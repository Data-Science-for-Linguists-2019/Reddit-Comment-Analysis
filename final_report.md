# Final Report

## Name : Matthew Borbonus
## Username : MJB288
## Date : 4/26/19

My project was a rather interesting one. First off, there was the issue of the corpus size. I initially wanted one month in size, but that was 40 GB uncompressed in size, much bigger than my RAM total's. While I knew it was possible that it could be processed in chunks, I also knew that not being able to have the whole Corpus in memory would slow me down. So I downsized the corpus to one day which was 2 GB uncompressed. Which was much better. With that out of the way, I was able to move onto cleaning the data, removing columns that I was either not interested in or did not seem important. 

I did endure one major setback though. All of my progress on phase 1 of the project was mostly for naught. [This](https://nbviewer.jupyter.org/github/Data-Science-for-Linguists-2019/Reddit-Comment-Analysis/blob/master/legacy_notebooks/project-explore.ipynb#corpus) for-loop/function is what I used to originally read in the 40 GB corpus size. Well, I switched corpus size last minute, but I kept using that function which was a very impractical way to do it (not to mention I was appending directly to a data frame which I found out later was incredibly slow). All of my data processing was for nothing as I did data cleanup for only one subreddit, not the entire data set. This is why I redid the data cleanup on phase 2 that I also did on phase 1. This is a more minor setback, but I published by first data sample incorrectly as well. I made the mistake of arranging it by hand with extremes and the middle of the data, instead of using the sample function for dataframes. Overall, phase 1 was the least productive of all the phases because none of the results ended up getting used. It is tied with this phase for productivity since I was unable to do much for this round.

In phase 2 I corrected these errors by [loading in the data directly with pd.read_json](https://nbviewer.jupyter.org/github/Data-Science-for-Linguists-2019/Reddit-Comment-Analysis/blob/master/legacy_notebooks/phase2_exploration.ipynb#dataacq). I could not expand upon this further with another day because I wanted the ability to work on the project on the go, and loading one day size of data would nearly max out my data. Moving on, after the cleaning up the data, I published a correct sample using the sample function instead of making the same mistake again. Thank you for directing me to the sample function.

I filtered my data to a total score value of 50 or greater because, it's an internet for

I used two models in my machine learning project, Naive Bayes and Support Vectors. I wanted to use more, but I was strapped for time. I did some [preliminary machine learning](https://nbviewer.jupyter.org/github/Data-Science-for-Linguists-2019/Reddit-Comment-Analysis/blob/master/legacy_notebooks/phase2_exploration.ipynb#machinelearn) to establish some baselines and to get an idea of what I was up against. I established a baseline accuracy of 11% against the whole corpus. I explore some subset of subreddits later to experiment with said groups, getting a feel for what subreddits to use to fine tune the models. Support vectors was superior to Naive Bayes in the trials where they were both run. This was not really surprising however. Here is an example heatmap of one combination of subreddits:

![png](images/lastsupportvectors.png) 

For phase 3, I settled upon 14 subreddits to fine tune my model settings to re-apply to the whole corpus. This list included the following subreddits: relationships, aww, nfl, PrequelMemes, gaming, mildlyinteresting, politics, Showerthoughts, worldnews, gifs, StarWars, and funny. I wanted to use AskReddit, but as I saw the [phase 2 basic stats section](https://nbviewer.jupyter.org/github/Data-Science-for-Linguists-2019/Reddit-Comment-Analysis/blob/master/legacy_notebooks/phase2_exploration.ipynb#stats), AskReddit has a severe disproportionate amount of data (6735 samples with the filter, 2nd place is ), and I know from offhand experience that many topics are discussed there, so selecting that subreddit would have made finding routes to optimize my models difficult. Nonetheless, I wanted some subreddit to be disproportional to the rest, so I selected politics, which was number 3 on that list (with 1786 samples), but less disproportional than AskReddit.  